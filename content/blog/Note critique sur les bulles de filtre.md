---
title: Note critique sur les bulles de filtre
date: 2024-03-24
tags:
  - "internet"
  - "sciences sociales"
description: This is a post
author: Yann Daout
---
Le concept de "bulle de filtre" est entré dans le language courant. L'idée derrière ce concept est que le contenu auquel nous avons accès sur internet est personnalisé en étant filtré par des algorithmes. Ce faisant, les algorithmes nous enferment dans des bulles ne contenant que ce qui correspond à notre personnalité (goûts, préférences, etc.). Un utilisateur d'extrême-gauche par exemple se verrait proposer un accès politiquement biaisé à l'information, mettant à distance toutes perspectives opposées à la sienne. Cette fragmentation informationnelle serait une cause de fragmentation politique et sociale. Si c'est vrai, c'est grave. Mais encore faudrait-il que ce soit vrai. Le concept de bulle de filtres a selon moi deux problèmes. Le premier est qu'il n'est pas étayé empiriquement, et donc semble globalement faux. C'est là peut-être plus une raison de se réjouir qu'un problème. Le second est qu'il masque d'autres mécanismes sociaux réellement à l'oeuvre non seulement sur les réseaux sociaux, mais également dans notre consommation de médias traditionnels. Le concept de bulle de filtre me semble donc trompeur et nuisible à une bonne compréhension du monde social, du moins tel qu'il est utilisé dans le language courant.

# Un concept séduisant...
Le concept de bulle de filtre est très séduisant. Cette séduction réside partiellement dans sa clarté. Le concept est imagé, le mécanisme proposé est relativement simple et élégant. Cette clarté conduit à l'adopter sans plus de réflexion. C'est ce que le philosophe C. Thi Nguyen appelle une heuristique d'arrêt de la pensée: le concept est suffisamment clair, il n'y pas besoin de réfléchir plus^[Nguyen, C. T. (2021). The Seductions of Clarity. _Royal Institute of Philosophy Supplements_, _89_, 227–255.]. Je me permet de dire cela parce que dans mon expérience, chaque fois que quelqu'un évoquait ce concept c'était sans s'appuyer sur des données empiriques. La première fois que j'ai rencontré le concept, c'était un prof de sociologie qui en parlait sur le ton de l'évidence: "comment, vous n'avez jamais entendu parler de bulles de filtres ?"
Le concept est également séduisant parce qu'il prend place sur un fond de panique morale sur les réseaux sociaux. Je pense qu'on a tous des exemples en tête d'entrepreneurs de morale qui essaient de susciter de l'inquiétude au sujet des "résosocio". Les bulles de filtres sont un vernis de scientificité et de sérieux sur ces paniques morales.
Je ne dis pas ici qu'il n'y a aucune raison de se soucier des usages que l'on peut faire d'internet. Mais lorsque les bulles de filtres sont évoquées, c'est généralement avec le message implicite ou explicite que les bulles de filtre sont effectives, qu'elles existent et sont répandues aujourd'hui. Si elles n'existent pas, c'est une mauvaise raison de s'indigner. Or, la recherche empirique incite à mettre en doute cet usage du concept.

# ... mais mal étayé empiriquement
A l'origine, le concept de bulle de filtre était plutôt centré sur les moteurs de recherche, le présupposé étant que deux utilisateurs d'un moteur de recherche verraient pour des mêmes mots clés des résultats drastiquement différents, l'enfermant ainsi dans une bulle assez étanche. Selon les données de différentes équipes de chercheurs, les différences sont en fait mineures et consistent essentiellement en des différences nationales^[Bruns, A. (2021). Echo chambers? Filter bubbles? The misleading metaphors that obscure the real problem. In _Hate speech and polarization in participatory society_ (pp. 33–48). Routledge.]. Autrement dit, des individus de pays différents verront des résultats différents. La théorie semble falsifiée empiriquement pour ce qui est des moteurs de recherche.

Pour ce qui est des réseaux sociaux, c'est plus difficile parce que le concept est plus difficile à appliquer. Après tout, sur la plupart des réseaux sociaux, les individus choisissent des communautés données et s'insèrent dans des réseaux de contacts spécifiques. Au sein de la communauté "Le golf n'existe pas", il y a peu de risques que je soit confronté à des affirmations de l'existence du golf. Mais la conclusion change complètement dès lors que mon usage de la plateforme dans son entier est considéré. Je compte malheureusement parmi mes contacts des gens qui croient encore en l'existence du golf, certains même pensent pratiquer ou avoir pratiqué ce "sport". Cela me met bien sûr en colère, mais je suis malgré moi souvent confronté à la supposée existence du golf. Plus sérieusement, des données suggèrent que les usagers des réseaux sociaux sont confrontés à une plus grande diversité de sources d'information que les non-utilisateurs ^[Fletcher, R., & Nielsen, R. K. (2018). Are people incidentally exposed to news on social media? A comparative analysis. _New Media & Society_, _20_(7), 2450–2468.]. Dans une revue de la littérature, Arguedas et al. estiment que l'usage de plateformes avec classement algorithmique "conduit les gens à des informations légèrement plus diversifiées - à l'opposé de ce que l'hypothèse de la bulle de filtre postule" ^[Arguedas, A. R., Robertson, C., Fletcher, R., & Nielsen, R. (2022). _Echo chambers, filter bubbles, and polarisation: A literature review_.]. Réfléchissons deux minutes. Si une personne écoutant les discours concernant les bulles de filtre en venait à se radicaliser et à abandonner tout usage des réseaux sociaux, il est possible qu'elle appauvrisse la diversité de ses sources d'information.

Cela ne signifie certainement pas que le concept de bulle de filtre est inintéressant. Les bulles de filtres sont *possibles*. Une objection serait de dire que les algorithmes ne sont pas assez performants, mais qu'un jour ils le seront, et ce jour là, il y aura des bulles de filtre. Peut-être. On peut continuer à faire de la recherche empirique, identifier des levier d'action pour les réduire, mettre des politiques en place pour éviter qu'elles adviennent, etc. Mais quoi qu'il en soit, le véritable problème est que l'utilisation actuelle du concept dans le language commun tend à masquer d'autres phénomènes qui eux sont réels maintenant.
# En quoi les algorithmes sont-ils spécifiques ?
Le fait est qu'il y a de la polarisation et de la fragmentation dans certaines communautés sur internet^[Bruns, A. (2021). Echo chambers? Filter bubbles? The misleading metaphors that obscure the real problem. In _Hate speech and polarization in participatory society_ (pp. 33–48). Routledge.]. Mais c'est là un fait tout à fait normal. On s'attend à ce qu'il y ait plus de gens de gauche dans un groupe politique d'extrême-gauche, que ce soit sur les réseaux sociaux... ou IRL.
Il est assez amusant de constater à quel point l'idée de bulle de filtre parait scandaleuse lorsqu'on parle d'internet, alors qu'on trouve par ailleurs tout à fait normal la fragmentation sociale et informationnelle. Prenez les autres médias. Si il y a une "bulle de filtre" par excellence, c'est le journal. Si je suis abonné à un magazine d'haltérophilie, il est peu probable que je rencontre souvent des informations sur l'ornithologie; et si je m'abonne au Monde Diplomatique, les informations auxquelles j'aurai accès (et leur qualité) seront bien différentes de celles d'un lecteur de Valeurs Actuelles. Or, personne ne lit toute la presse à moins d'être éditorialiste. Lorsque nous nous abonnons à un journal, nous optons pour un fragment de l'information, à l'exclusion d'autres. Est-ce là un scandale ? En tout cas, cela n'est rien de neuf.

Plus généralement, il faut bien voir que la fragmentation sociale précède la fragmentation informationnelle. Nous naissons et évoluons dans des groupes sociaux spécifiques. Les enfants traînent généralement avec d'autres enfants; les hommes fréquentent des espaces de sociabilité masculine; les bourgeois s'efforcent visiblement de mettre à distance les prolétaires; les fachos ont leurs bars et salles de boxe qu'on peut suspecter d'être inhospitaliers aux antifas; etc. Lorsqu'il y a de l'homosociabilité sur internet, c'est un simple prolongement de cela.

Pour être encore plus clair, pour insister sur la spécificité des algorithmes il faudrait montrer que ces derniers soient d'une certaine manière pires que la fragmentation sociale préexistante. Mais la probabilité que je tombe sur une perspective éloignée de la mienne est-elle vraiment *plus faible* lorsque la recommandation est faite par un algorithme plutôt que par un ami ? L'information sur laquelle je tombe sur internet m'enferme-t-elle plus dans une bulle de filtre que celle provenant du journal acheté par une personne du même milieu social que moi, mettons un collègue ou un proche ? Ce que je veux dire, c'est que notre milieu social filtre déjà notre accès à l'information, il faudrait qu'internet soit un filtre plus strict encore avant que la panique soit justifiée. Mais comme on l'a vu, la recherche empirique suggère plutôt le contraire.
# Les mécanismes alternatifs
Il faut bien voir que derrière l'idée de bulle de filtre, la responsabilité est mise sur les algorithmes plutôt que sur d'autres mécanismes. Mais il y a bien des mécanismes qui peuvent conduire à un appauvrissement de la diversité des sources d'information disponibles sur internet. Les cas de radicalisation où des individus se coupent de toute source d'informations contradictoires existent certainement. Mais dans ce cas, il y a une foule de mécanismes qui entrent en jeu, qui sont complètement effacés dès lors qu'on se contente de parler de bulle de filtre. L'un deux est simplement que nous importons sur internet notre réseau de contacts IRL, qui reproduit le filtre de notre milieu social. Un autre est l'adoption explicite d'une communauté ou d'un réseau en particulier. On peut citer l'exemple de Parler, l'alternative conservatrice à Twitter (disons, avant le déclin de Twitter). Une plateforme peut mettre en avant certains contenus pour des raisons idéologiques ou tout simplement financières et présenter ainsi un biais. Mais cela n'est pas équivalent à une bulle de filtre personnalisée, et est similaire à l'exemple de la souscription à un journal.

Un autre mécanisme est que les utilisateurs filtrent eux-mêmes le contenu, volontairement ou involontairement, consciemment ou non. Si on ouvre un peu les yeux, on verra que le concept de bulle de filtre colle assez mal avec notre expérience des algorithmes. Du moins, il est tout à fait inadéquat pour rendre compte de ma propre expérience, aussi anecdotique soit-elle. Est-ce que les recommandations algorithmiques sont vraiment alignées sur mes "goûts" "préférences" ou "personnalité" comme on peut parfois l'entendre ? Certainement pas. Je dois filtrer manuellement ou fermer les yeux sur une masse effarante de contenu inepte. YouTube devrait savoir que je ne souhaite rien de plus que regarder des memes de Skyrim, et pourtant les memes sont absents de mes recommandations. Il faut être clair: de manière générale, les algorithmes ne sont pas si personnalisés que cela, et surtout il semble que le but est plutôt de me faire cliquer sur du contenu qui apporte de l'argent à la plateforme plutôt que de me proposer le contenu qui correspond à mes préférences. Cela ne remet pas en cause le concept de bulle de filtre: rien n'empêche d'enfermer quelqu'un dans une bulle qui ne correspond pas à ses préférences. Mais il y a une sorte d'esbrouffe dans la manière dont le concept est utilisé qui s'aligne sur les intérêts des grandes plateformes: celle qui consiste à nous faire croire que ce qui nous est recommandé par les algorithmes correspond à nos préférences, que c'est là la fonction des algorithmes et qu'ils sont en cela efficaces. 

Finalement, notons aussi une contradiction entre l'idée d'une bulle enfermant les individus dans leurs préférences idéologiques et celle, fréquemment remarquée, qu'internet fonctionne d'une manière ou d'une autre sur une forme de polarisation émotionnelle. Je pense ici à l'indignation, la colère, la panique morale, la culture du clash, etc. Alors certes, il n'est pas impossible d'enfermer quelqu'un idéologiquement avec des hommes de paille. On le voit bien avec toutes les paniques morales du moment. Mais il me semble qu'il y a bien une dynamique consistant à exploiter notre propension à cliquer sur du contenu radicalement opposé à nos préférences idéologiques ou à nos croyances. Je n'ai aucune idée de l'ampleur de ce phénomène, mais il me semble qu'il s'agit là d'un phénomène opposé au concept de bulle de filtre tel qu'il est habituellement utilisé. Un vidéaste de gauche parlait récemment du fait que ses vidéos les plus "populaires" l'étaient en grande partie parce qu'elles attiraient un public de droite, venu envahir ses commentaires. Je ne dis pas qu'il s'agit là d'un public épistémiquement prédisposé à la controverse et intéressé à la délibération. Au contraire, c'est même parfois par pure volonté de se moquer que l'on clique sur une vidéo. On voit bien que le problème de la polarisation n'est pas du tout réglé par la simple exposition.

Pour résumer: le concept de bulle de filtre est séduisant, mais cette séduction nous incite à arrêter de réfléchir à un phénomène social complexe. Les données empiriques actuelles tendent à infirmer l'existence de bulles de filtre. Alors que le concept a prétention d'expliquer quelque chose, il participe de fait à masquer certains mécanismes sociaux. Surtout, il masque le fait que les problèmes ne sont pas essentiellement technologiques, et que leurs solutions... ne peuvent donc pas être essentiellement technologiques. Bien sûr, il reste la possibilité que toutes mes recherches sur le sujet se soient faites dans une bulle de filtre, et que vous qui me lisez le soyez également.




